import os
import requests
import re
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from langchain.schema import Document
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_huggingface import HuggingFaceEndpoint
import streamlit as st

# ğŸ“Œ DÃ©finir votre jeton Hugging Face Ã  partir des variables d'environnement
HF_TOKEN = os.getenv("HF_TOKEN")
if not HF_TOKEN:
    st.error("âš ï¸ Hugging Face token non dÃ©fini ! VÃ©rifiez vos variables d'environnement.")
    st.stop()

# ğŸ“Œ DÃ©finition du modÃ¨le IA
MODEL_ID = "mistralai/Mistral-7B-Instruct-v0.3"

# ğŸ“Œ Chargement du modÃ¨le Hugging Face
llm = HuggingFaceEndpoint(
    repo_id=MODEL_ID,
    task="text-generation",
    huggingfacehub_api_token=HF_TOKEN,
    max_new_tokens=1024,
    do_sample=True,
    temperature=0.95,
    top_p=0.95
)

# ğŸ“Œ Fonction pour scraper les articles
def scrape_articles(url):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()  # VÃ©rifie les erreurs HTTP
    except requests.RequestException as e:
        st.error(f"ğŸš¨ Erreur lors de l'accÃ¨s Ã  {url} : {e}")
        return []

    soup = BeautifulSoup(response.content, 'html.parser')
    links = {a['href'] for a in soup.find_all('a', href=True)}

    articles = []
    for link in links:
        if any(social in link for social in ["facebook", "twitter", "linkedin", "instagram", "mailto"]):
            continue

        full_link = urljoin(url, link)

        try:
            article_response = requests.get(full_link, timeout=10)
            article_response.raise_for_status()
            article_soup = BeautifulSoup(article_response.content, 'html.parser')

            title = article_soup.find('h1')
            title = title.get_text(strip=True) if title else "Titre non trouvÃ©"

            content = " ".join([p.get_text(strip=True) for p in article_soup.find_all('p')])
            if content:
                articles.append(Document(page_content=f"{title}\n\n{content}", metadata={"url": full_link}))
        except requests.RequestException as e:
            st.error(f"âŒ Impossible de rÃ©cupÃ©rer {full_link} : {e}")

    return articles

# ğŸ“Œ Nettoyage du texte
def clean_text(text):
    text = re.sub(r'<.*?>', '', text)  # Supprime les balises HTML
    text = re.sub(r'\s+', ' ', text).strip()  # Supprime les espaces inutiles
    return text

# ğŸ“Œ PrÃ©traitement des articles
def preprocess_articles(documents):
    return [Document(page_content=clean_text(doc.page_content), metadata=doc.metadata) for doc in documents]

# ğŸ“Œ URL de la page Ã  scraper
base_url = "https://www.agenceecofin.com/"
documents = scrape_articles(base_url)

# ğŸ“Œ VÃ©rification et traitement des articles
if not documents:
    st.warning("âš ï¸ Aucun article n'a Ã©tÃ© rÃ©cupÃ©rÃ©. VÃ©rifiez l'URL ou le site source.")
else:
    processed_articles = preprocess_articles(documents)

    # ğŸ“Œ CrÃ©ation des embeddings pour le RAG
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # ğŸ“Œ Indexation des documents dans FAISS
    vectorstore = FAISS.from_documents(processed_articles, embeddings)

    # ğŸ“Œ ChaÃ®ne de questions-rÃ©ponses avec rÃ©cupÃ©ration
    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())

    # ğŸ“Œ Interface Streamlit
    st.title("Chatbot RAG")
    st.write("Posez vos questions, et le chatbot y rÃ©pondra en s'appuyant sur des articles scrappÃ©s.")
    question = st.text_input("Votre question :")
    
    if question:
        reponse = qa_chain.run(question)
        st.write(f"ğŸ“ RÃ©ponse : {reponse}")
